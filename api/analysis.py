import re
from collections import Counter
from textblob import TextBlob
from deep_translator import GoogleTranslator
import math

def analyze_sentiment(transcript):
    sentiments = {"positive": 0, "neutral": 0, "negative": 0}
    total = len(transcript)

    for item in transcript:
        try:
            translated = GoogleTranslator(source='ar', target='en').translate(item['text'])
            polarity = TextBlob(translated).sentiment.polarity
            if polarity > 0.1:
                sentiments["positive"] += 1
            elif polarity < -0.1:
                sentiments["negative"] += 1
            else:
                sentiments["neutral"] += 1
        except:
            sentiments["neutral"] += 1

    for k in sentiments:
        sentiments[k] = f"{round((sentiments[k] / total) * 100, 2)}%"
    return sentiments

def analyze_psychological_patterns(transcript):
    """ØªØ­Ù„ÙŠÙ„ Ù†ÙØ³ÙŠ Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ© ÙˆØ§Ù„Ø´Ø®ØµÙŠØ©"""
    text = " ".join([item['text'] for item in transcript])
    total_words = len(text.split())
    
    # Ù…Ø¤Ø´Ø±Ø§Øª Ù†ÙØ³ÙŠØ©
    confidence_indicators = ["Ø£Ø¹ØªÙ‚Ø¯", "Ø£Ø¤ÙƒØ¯", "Ù…ØªØ£ÙƒØ¯", "Ø¨Ø§Ù„ØªØ£ÙƒÙŠØ¯", "Ø¨Ø«Ù‚Ø©", "Ø£Ø¹Ø±Ù", "Ù…Ù‚ØªÙ†Ø¹"]
    uncertainty_indicators = ["Ø±Ø¨Ù…Ø§", "Ù‚Ø¯", "Ù…Ù…ÙƒÙ†", "Ù„Ø§ Ø£Ø¹Ø±Ù", "ØºÙŠØ± Ù…ØªØ£ÙƒØ¯", "Ø£Ø¸Ù†", "ÙŠØ¨Ø¯Ùˆ"]
    stress_indicators = ["ØªÙˆØªØ±", "Ù‚Ù„Ù‚", "Ø®ÙˆÙ", "Ø¶ØºØ·", "Ù…Ø´ÙƒÙ„Ø©", "ØµØ¹Ø¨", "Ù…Ø¤Ù„Ù…", "Ø£Ø²Ù…Ø©"]
    emotional_words = ["Ø­Ø¨", "ÙØ±Ø­", "Ø­Ø²Ù†", "ØºØ¶Ø¨", "Ø®ÙˆÙ", "Ø£Ù…Ù„", "ÙŠØ£Ø³", "Ø³Ø¹Ø§Ø¯Ø©"]
    
    confidence_score = sum(1 for word in confidence_indicators if word in text)
    uncertainty_score = sum(1 for word in uncertainty_indicators if word in text)
    stress_level = sum(1 for word in stress_indicators if word in text)
    emotional_intensity = sum(1 for word in emotional_words if word in text)
    
    # ØªØ­Ù„ÙŠÙ„ Ø³Ø±Ø¹Ø© Ø§Ù„ÙƒÙ„Ø§Ù… Ø§Ù„Ù†ÙØ³ÙŠØ©
    speech_rate = calculate_overall_speech_rate(transcript)
    
    psychological_profile = {
        "confidence_level": min(100, (confidence_score / max(1, uncertainty_score)) * 20),
        "stress_indicators": stress_level,
        "emotional_stability": max(0, 100 - (stress_level * 10)),
        "communication_clarity": min(100, (total_words / len(transcript)) * 5) if len(transcript) > 0 else 0,
        "speech_pattern": {
            "rate": speech_rate,
            "pattern_type": "Ù…ØªØ³Ø§Ø±Ø¹" if speech_rate > 2.5 else "Ø¨Ø·ÙŠØ¡" if speech_rate < 1.5 else "Ø·Ø¨ÙŠØ¹ÙŠ",
            "psychological_meaning": get_speech_pattern_meaning(speech_rate)
        }
    }
    
    return psychological_profile

def get_speech_pattern_meaning(rate):
    """ØªÙØ³ÙŠØ± Ù†ÙØ³ÙŠ Ù„Ø³Ø±Ø¹Ø© Ø§Ù„ÙƒÙ„Ø§Ù…"""
    if rate > 3.0:
        return "Ø³Ø±Ø¹Ø© Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹ - Ù‚Ø¯ ØªØ´ÙŠØ± Ø¥Ù„Ù‰ ØªÙˆØªØ± Ø£Ùˆ Ø­Ù…Ø§Ø³ Ø´Ø¯ÙŠØ¯"
    elif rate > 2.5:
        return "Ø³Ø±Ø¹Ø© Ø¹Ø§Ù„ÙŠØ© - ÙŠØ¯Ù„ Ø¹Ù„Ù‰ Ù†Ø´Ø§Ø· Ø°Ù‡Ù†ÙŠ ÙˆØ«Ù‚Ø© Ø¨Ø§Ù„Ù†ÙØ³"
    elif rate > 2.0:
        return "Ø³Ø±Ø¹Ø© Ø·Ø¨ÙŠØ¹ÙŠØ© Ù…Ø±ØªÙØ¹Ø© - Ø´Ø®ØµÙŠØ© Ù†Ø´Ø·Ø© ÙˆÙ…ØªÙØ§Ø¹Ù„Ø©"  
    elif rate > 1.5:
        return "Ø³Ø±Ø¹Ø© Ø·Ø¨ÙŠØ¹ÙŠØ© - ØªÙˆØ§Ø²Ù† Ø¬ÙŠØ¯ ÙÙŠ Ø§Ù„ØªÙˆØ§ØµÙ„"
    elif rate > 1.0:
        return "Ø³Ø±Ø¹Ø© Ù…Ù†Ø®ÙØ¶Ø© - Ù‚Ø¯ ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ø§Ù„ØªØ£Ù†ÙŠ ÙˆØ§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ø¹Ù…ÙŠÙ‚"
    else:
        return "Ø³Ø±Ø¹Ø© Ù…Ù†Ø®ÙØ¶Ø© Ø¬Ø¯Ø§Ù‹ - Ù‚Ø¯ ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ ØªØ±Ø¯Ø¯ Ø£Ùˆ Ù‚Ù„Ù‚"

def analyze_deception_indicators(transcript):
    """ØªØ­Ù„ÙŠÙ„ Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø®Ø¯Ø§Ø¹ ÙˆØ§Ù„ØµØ¯Ù‚"""
    deception_words = [
        "ÙÙŠ Ø§Ù„ÙˆØ§Ù‚Ø¹", "ØµØ¯Ù‚Ù†ÙŠ", "Ø¨ØµØ±Ø§Ø­Ø©", "ÙˆØ§Ù„Ù„Ù‡", "Ø£Ù‚Ø³Ù…", 
        "Ù„Ø§ Ø£ÙƒØ°Ø¨", "Ø­Ù‚ÙŠÙ‚Ø©", "Ø£Ø¤ÙƒØ¯ Ù„Ùƒ", "Ø¨Ø§Ù„ÙØ¹Ù„", "Ø­Ù‚Ø§Ù‹"
    ]
    
    hesitation_patterns = [
        "Ø¢Ø¢Ø¢", "Ø¥Ù…Ù…Ù…", "ÙŠØ¹Ù†ÙŠ", "ÙƒÙŠÙ Ø£Ù‚ÙˆÙ„", "Ø£Ù‚ØµØ¯", 
        "Ø¨Ù…Ø¹Ù†Ù‰", "Ø£ÙŠ", "Ø£Ù„Ø§ ÙˆÙ‡Ùˆ", "ÙƒÙ…Ø§ ØªØ¹Ù„Ù…"
    ]
    
    text = " ".join([item['text'] for item in transcript])
    
    deception_score = sum(1 for word in deception_words if word in text)
    hesitation_score = sum(1 for pattern in hesitation_patterns if pattern in text)
    
    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„Ù…ÙØ±Ø·
    word_frequency = Counter(text.split())
    repetition_score = sum(1 for count in word_frequency.values() if count > 5)
    
    credibility_analysis = {
        "deception_indicators": deception_score,
        "hesitation_patterns": hesitation_score,
        "repetition_score": repetition_score,
        "credibility_rating": max(0, 100 - (deception_score * 5 + hesitation_score * 3 + repetition_score * 2)),
        "analysis_notes": get_credibility_notes(deception_score, hesitation_score, repetition_score)
    }
    
    return credibility_analysis

def get_credibility_notes(deception, hesitation, repetition):
    """ØªÙ‚Ø¯ÙŠÙ… Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø­ÙˆÙ„ Ø§Ù„Ù…ØµØ¯Ø§Ù‚ÙŠØ©"""
    notes = []
    
    if deception > 3:
        notes.append("Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙØ±Ø· Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªØ£ÙƒÙŠØ¯ Ù‚Ø¯ ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥Ù‚Ù†Ø§Ø¹")
    if hesitation > 5:
        notes.append("ÙˆØ¬ÙˆØ¯ ØªØ±Ø¯Ø¯ ÙˆØ§Ø¶Ø­ ÙÙŠ Ø§Ù„ÙƒÙ„Ø§Ù…")
    if repetition > 3: 
        notes.append("ØªÙƒØ±Ø§Ø± Ù…ÙØ±Ø· Ù„Ø¨Ø¹Ø¶ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù‚Ø¯ ÙŠØ¯Ù„ Ø¹Ù„Ù‰ ØªÙˆØªØ±")
    
    if not notes:
        notes.append("Ø£Ù†Ù…Ø§Ø· ÙƒÙ„Ø§Ù… Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¯ÙˆÙ† Ù…Ø¤Ø´Ø±Ø§Øª ÙˆØ§Ø¶Ø­Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø®Ø¯Ø§Ø¹")
        
    return notes

def analyze_personality_traits(transcript):
    """ØªØ­Ù„ÙŠÙ„ Ø³Ù…Ø§Øª Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ø®Ù…Ø³ Ø§Ù„ÙƒØ¨Ø±Ù‰"""
    text = " ".join([item['text'] for item in transcript])
    
    # Ø§Ù„Ø§Ù†ÙØªØ§Ø­ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¬Ø±Ø¨Ø©
    openness_words = ["Ø¬Ø¯ÙŠØ¯", "Ù…Ø®ØªÙ„Ù", "Ø¥Ø¨Ø¯Ø§Ø¹", "ÙÙƒØ±Ø©", "ØªØ¬Ø±Ø¨Ø©", "Ù…ØºØ§Ù…Ø±Ø©", "ØªØºÙŠÙŠØ±"]
    
    # Ø§Ù„Ø¶Ù…ÙŠØ± Ø§Ù„Ø­ÙŠ
    conscientiousness_words = ["Ù…Ù†Ø¸Ù…", "Ø¯Ù‚ÙŠÙ‚", "Ù…Ø³Ø¤ÙˆÙ„", "Ø§Ù„ÙˆÙ‚Øª", "Ø®Ø·Ø©", "Ù‡Ø¯Ù", "Ø¥Ù†Ø¬Ø§Ø²"]
    
    # Ø§Ù„Ø§Ù†Ø¨Ø³Ø§Ø·
    extraversion_words = ["Ø§Ø¬ØªÙ…Ø§Ø¹", "Ù†Ø§Ø³", "Ø£ØµØ¯Ù‚Ø§Ø¡", "Ø­ÙÙ„Ø©", "Ù†Ø´Ø§Ø·", "Ø·Ø§Ù‚Ø©", "Ù…ØªØ­Ù…Ø³"]
    
    # Ø§Ù„ÙˆØ¯Ø§Ø¹Ø©
    agreeableness_words = ["Ù…Ø³Ø§Ø¹Ø¯Ø©", "ØªØ¹Ø§ÙˆÙ†", "Ù„Ø·ÙŠÙ", "ØµØ¨Ø±", "ØªÙÙ‡Ù…", "Ø§Ø­ØªØ±Ø§Ù…", "Ø³Ù„Ø§Ù…"]
    
    # Ø§Ù„Ø¹ØµØ§Ø¨ÙŠØ©
    neuroticism_words = ["Ù‚Ù„Ù‚", "ØªÙˆØªØ±", "Ø®ÙˆÙ", "Ø­Ø²Ù†", "ØºØ¶Ø¨", "Ø¶ØºØ·", "Ù…Ø´ÙƒÙ„Ø©"]
    
    traits = {
        "openness": sum(1 for word in openness_words if word in text),
        "conscientiousness": sum(1 for word in conscientiousness_words if word in text), 
        "extraversion": sum(1 for word in extraversion_words if word in text),
        "agreeableness": sum(1 for word in agreeableness_words if word in text),
        "neuroticism": sum(1 for word in neuroticism_words if word in text)
    }
    
    # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù†Ø³Ø¨ Ù…Ø¦ÙˆÙŠØ©
    total_indicators = sum(traits.values()) or 1
    personality_profile = {
        trait: round((score / total_indicators) * 100, 1) 
        for trait, score in traits.items()
    }
    
    # Ø¥Ø¶Ø§ÙØ© ØªÙØ³ÙŠØ±Ø§Øª
    personality_profile["dominant_trait"] = max(personality_profile.items(), key=lambda x: x[1])
    personality_profile["personality_summary"] = get_personality_summary(personality_profile)
    
    return personality_profile

def get_personality_summary(profile):
    """Ù…Ù„Ø®Øµ Ø§Ù„Ø´Ø®ØµÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„"""
    dominant = profile["dominant_trait"][0]
    
    summaries = {
        "openness": "Ø´Ø®ØµÙŠØ© Ù…Ù†ÙØªØ­Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªØ¬Ø§Ø±Ø¨ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ÙˆÙ…Ø¨Ø¯Ø¹Ø©",
        "conscientiousness": "Ø´Ø®ØµÙŠØ© Ù…Ù†Ø¸Ù…Ø© ÙˆÙ…Ø³Ø¤ÙˆÙ„Ø© ÙˆÙ…Ù„ØªØ²Ù…Ø©",
        "extraversion": "Ø´Ø®ØµÙŠØ© Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ© ÙˆÙ†Ø´Ø·Ø© ÙˆÙ…ØªÙØ§Ø¹Ù„Ø©",
        "agreeableness": "Ø´Ø®ØµÙŠØ© ÙˆØ¯ÙˆØ¯Ø© ÙˆÙ…ØªØ¹Ø§ÙˆÙ†Ø© ÙˆÙ…ØªÙÙ‡Ù…Ø©", 
        "neuroticism": "Ø´Ø®ØµÙŠØ© Ø­Ø³Ø§Ø³Ø© ÙˆÙ‚Ø¯ ØªØ­ØªØ§Ø¬ Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¶ØºÙˆØ·"
    }
    
    return summaries.get(dominant, "Ø´Ø®ØµÙŠØ© Ù…ØªÙˆØ§Ø²Ù†Ø©")

def translate_to_english(transcript):
    """ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø¥Ù„Ù‰ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø§Ù„ÙØ¹Ù„ÙŠØ©"""
    translations = []
    for t in transcript:
        try:
            # ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø¥Ù„Ù‰ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
            translated_text = GoogleTranslator(source='ar', target='en').translate(t['text'])
            translations.append({
                "start": t["start"],
                "end": t["end"],
                "arabic_text": t['text'],  # Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ø£ØµÙ„ÙŠ
                "english_text": translated_text  # Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
            })
        except Exception as e:
            print(f"Translation error: {e}")
            # ÙÙŠ Ø­Ø§Ù„Ø© ÙØ´Ù„ Ø§Ù„ØªØ±Ø¬Ù…Ø©ØŒ Ø§Ø­ØªÙØ¸ Ø¨Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ Ù…Ø¹ Ù…Ù„Ø§Ø­Ø¸Ø©
            translations.append({
                "start": t["start"], 
                "end": t["end"],
                "arabic_text": t['text'],
                "english_text": f"[Translation failed] Please check internet connection"
            })
    return translations

def analyze_word_repetition(transcript):
    """ØªØ­Ù„ÙŠÙ„ ØªÙƒØ±Ø§Ø± Ø§Ù„ÙƒÙ„Ø§Ù… ÙˆÙ…Ø§ ÙŠØ¯Ù„ Ø¹Ù„ÙŠÙ‡ Ù†ÙØ³ÙŠØ§Ù‹"""
    text = " ".join([item['text'] for item in transcript])
    words = text.split()
    word_frequency = Counter(words)
    
    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙƒØ±Ø§Ø±
    total_words = len(words)
    unique_words = len(set(words))
    repetition_ratio = (total_words - unique_words) / total_words if total_words > 0 else 0
    
    # Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ø§Ù‹
    most_repeated = word_frequency.most_common(10)
    
    # ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªÙƒØ±Ø§Ø±
    excessive_repetition = [word for word, count in word_frequency.items() if count > 5 and len(word) > 2]
    
    # Ø§Ù„ØªÙØ³ÙŠØ± Ø§Ù„Ù†ÙØ³ÙŠ Ù„Ù„ØªÙƒØ±Ø§Ø±
    psychological_meaning = get_repetition_psychological_meaning(repetition_ratio, excessive_repetition)
    
    return {
        "repetition_ratio": round(repetition_ratio * 100, 2),
        "total_words": total_words,
        "unique_words": unique_words,
        "most_repeated_words": most_repeated,
        "excessive_repetition": excessive_repetition,
        "psychological_analysis": psychological_meaning,
        "repetition_level": get_repetition_level(repetition_ratio)
    }

def get_repetition_psychological_meaning(ratio, excessive_words):
    """Ø§Ù„ØªÙØ³ÙŠØ± Ø§Ù„Ù†ÙØ³ÙŠ Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„ÙƒÙ„Ø§Ù…"""
    meanings = []
    
    if ratio > 0.4:  # ØªÙƒØ±Ø§Ø± Ø¹Ø§Ù„ÙŠ
        meanings.append("ØªÙƒØ±Ø§Ø± Ø¹Ø§Ù„ÙŠ Ù‚Ø¯ ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰:")
        meanings.append("â€¢ ØªÙˆØªØ± Ø£Ùˆ Ù‚Ù„Ù‚ Ø´Ø¯ÙŠØ¯")
        meanings.append("â€¢ Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ØªØ£ÙƒÙŠØ¯ Ø¹Ù„Ù‰ Ù†Ù‚Ø·Ø© Ù…Ø¹ÙŠÙ†Ø©")
        meanings.append("â€¢ Ù†Ù‚Øµ ÙÙŠ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª Ø£Ùˆ ØµØ¹ÙˆØ¨Ø© ÙÙŠ Ø§Ù„ØªØ¹Ø¨ÙŠØ±")
        
    elif ratio > 0.25:  # ØªÙƒØ±Ø§Ø± Ù…ØªÙˆØ³Ø·
        meanings.append("ØªÙƒØ±Ø§Ø± Ù…ØªÙˆØ³Ø· Ù‚Ø¯ ÙŠØ¯Ù„ Ø¹Ù„Ù‰:")
        meanings.append("â€¢ Ø±ØºØ¨Ø© ÙÙŠ Ø§Ù„ÙˆØ¶ÙˆØ­ ÙˆØ§Ù„ØªØ£ÙƒÙŠØ¯")
        meanings.append("â€¢ ØªÙÙƒÙŠØ± Ù…Ù†Ø¸Ù… ÙˆÙ…ØªØ³Ù„Ø³Ù„")
        meanings.append("â€¢ Ø­Ø±Øµ Ø¹Ù„Ù‰ Ø¥ÙŠØµØ§Ù„ Ø§Ù„Ù…Ø¹Ù†Ù‰ Ø¨Ø¯Ù‚Ø©")
        
    else:  # ØªÙƒØ±Ø§Ø± Ù…Ù†Ø®ÙØ¶
        meanings.append("ØªÙƒØ±Ø§Ø± Ù…Ù†Ø®ÙØ¶ ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰:")
        meanings.append("â€¢ Ø«Ø±Ø§Ø¡ ÙÙŠ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª")
        meanings.append("â€¢ Ø«Ù‚Ø© ÙÙŠ Ø§Ù„ØªØ¹Ø¨ÙŠØ±")
        meanings.append("â€¢ ØªÙ†ÙˆØ¹ ÙÙŠ Ø£Ø³Ø§Ù„ÙŠØ¨ Ø§Ù„ÙƒÙ„Ø§Ù…")
    
    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø© Ø¨Ø¥ÙØ±Ø§Ø·
    if len(excessive_words) > 3:
        meanings.append("\nÙƒÙ„Ù…Ø§Øª Ù…ÙƒØ±Ø±Ø© Ø¨Ø¥ÙØ±Ø§Ø·:")
        for word in excessive_words[:5]:
            meanings.append(f"â€¢ '{word}' - Ù‚Ø¯ ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ ØªØ±ÙƒÙŠØ² Ù…ÙØ±Ø· Ø¹Ù„Ù‰ Ù…ÙˆØ¶ÙˆØ¹ Ù…Ø¹ÙŠÙ†")
    
    return meanings

def get_repetition_level(ratio):
    """ØªØ­Ø¯ÙŠØ¯ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙƒØ±Ø§Ø±"""
    if ratio > 0.4:
        return "Ø¹Ø§Ù„ÙŠ"
    elif ratio > 0.25:
        return "Ù…ØªÙˆØ³Ø·"
    elif ratio > 0.15:
        return "Ø·Ø¨ÙŠØ¹ÙŠ"
    else:
        return "Ù…Ù†Ø®ÙØ¶"

def count_total_words(transcript):
    return sum(len(item['text'].split()) for item in transcript)
    return sum(len(item['text'].split()) for item in transcript)

def get_frequent_words(transcript, top_n=10):
    text = " ".join([item['text'] for item in transcript])
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
    stop_words = ["ÙÙŠ", "Ù…Ù†", "Ø¹Ù„Ù‰", "Ø¥Ù„Ù‰", "Ø¹Ù†", "Ù…Ø¹", "Ù‡Ø°Ø§", "Ù‡Ø°Ù‡", "Ø°Ù„Ùƒ", "Ø§Ù„ØªÙŠ", "Ø§Ù„Ø°ÙŠ", "Ùˆ", "Ø£Ù†", "Ù„Ø§", "Ù…Ø§", "ÙƒØ§Ù†", "ÙƒØ§Ù†Øª"]
    words = [word for word in re.findall(r'\b\w+\b', text) if word not in stop_words and len(word) > 2]
    counter = Counter(words)
    return counter.most_common(top_n)

def calculate_overall_speech_rate(transcript):
    total_words = sum(len(item['text'].split()) for item in transcript)
    total_duration = sum(item['end'] - item['start'] for item in transcript)
    return total_words / total_duration if total_duration > 0 else 0.0

def detect_sensitive_words(transcript, sensitive_words=None):
    if sensitive_words is None:
        sensitive_words = [
            "ØªÙ‡Ø¯ÙŠØ¯", "Ø³Ù„Ø§Ø­", "Ø®ÙŠØ§Ù†Ø©", "ØªÙØ¬ÙŠØ±", "Ø§ØºØªÙŠØ§Ù„", "Ù‚ØªÙ„",
            "Ø¹Ù†Ù", "Ø¥Ø±Ù‡Ø§Ø¨", "Ù…Ø¤Ø§Ù…Ø±Ø©", "Ø§Ø¹ØªØ¯Ø§Ø¡", "Ø§Ù†ÙØ¬Ø§Ø±", "Ø±ØµØ§Øµ",
            "Ù‡Ø¬ÙˆÙ…", "ÙƒÙ…ÙŠÙ†", "Ø­Ø±Ø¨", "ØµØ±Ø§Ø¹", "Ø¯Ù…Ø§Ø±", "Ø°Ø¨Ø­", "Ù…Ø³Ø¯Ø³",
            "Ù…Ø¬Ø²Ø±Ø©", "Ù…ÙˆØª", "Ù…Ù‡Ø§Ø¬Ù…Ø©", "ØªÙØ®ÙŠØ®", "Ù‚Ù†Øµ", "Ø¹Ø¯ÙˆØ§Ù†",
            "Ø®Ø·Ù", "Ø§Ø¨ØªØ²Ø§Ø²", "Ø¬Ø±ÙŠÙ…Ø©", "Ø¯Ù…Ø§Ø¡", "Ø¥ØµØ§Ø¨Ø©", "Ù…Ø¹Ø±ÙƒØ©",
            "Ù‚Ù†Ø¨Ù„Ø©", "Ø¹Ø¨ÙˆØ© Ù†Ø§Ø³ÙØ©", "Ø§Ø¬ØªÙŠØ§Ø­", "Ø¹ØµØ§Ø¨Ø©", "ØªØ­Ø±ÙŠØ¶"
        ]
    alerts = []
    for item in transcript:
        for word in sensitive_words:
            if word in item['text']:
                alerts.append({
                    "start": item["start"],
                    "end": item["end"],
                    "word": word,
                    "text": item['text']
                })
    return alerts

def generate_comprehensive_report(analysis_data, transcript):
    """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„ Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ÙØ³ÙŠ ÙˆØ§Ù„Ø¬Ù†Ø§Ø¦ÙŠ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""
    
    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
    speech_rate = analysis_data.get('speech_rate_wps', 0)
    total_words = analysis_data.get('total_words', 0)
    sentiment = analysis_data.get('sentiment', {})
    repetition_data = analysis_data.get('word_repetition_analysis', {})
    
    positive_pct = float(sentiment.get('positive', '0%').replace('%', ''))
    negative_pct = float(sentiment.get('negative', '0%').replace('%', ''))
    
    report = {
        "executive_summary": f"""
ğŸ“‹ Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠ Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ÙØ³ÙŠ ÙˆØ§Ù„Ø¬Ù†Ø§Ø¦ÙŠ:

ğŸ” Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©:
â€¢ ØªÙ… ØªØ­Ù„ÙŠÙ„ Ø¹ÙŠÙ†Ø© ØµÙˆØªÙŠØ© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ {total_words} ÙƒÙ„Ù…Ø©
â€¢ Ù…Ø¹Ø¯Ù„ Ø§Ù„ÙƒÙ„Ø§Ù…: {speech_rate:.2f} ÙƒÙ„Ù…Ø© ÙÙŠ Ø§Ù„Ø«Ø§Ù†ÙŠØ©
â€¢ Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©: {positive_pct}%
â€¢ Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø³Ù„Ø¨ÙŠØ©: {negative_pct}%
â€¢ Ù†Ø³Ø¨Ø© ØªÙƒØ±Ø§Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {repetition_data.get('repetition_ratio', 0)}%

ğŸ¯ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø´Ø§Ù…Ù„:
{'âœ… Ø´Ø®ØµÙŠØ© Ù…ØªÙØ§Ø¦Ù„Ø© ÙˆÙ…Ø³ØªÙ‚Ø±Ø© Ù†ÙØ³ÙŠØ§Ù‹' if positive_pct > 60 else 'âš ï¸ ÙŠØ­ØªØ§Ø¬ Ù„Ø¯Ø¹Ù… Ù†ÙØ³ÙŠ ÙˆÙ…ØªØ§Ø¨Ø¹Ø©' if negative_pct > 40 else 'ğŸ“Š Ø´Ø®ØµÙŠØ© Ù…ØªÙˆØ§Ø²Ù†Ø© Ù†Ø³Ø¨ÙŠØ§Ù‹'}

ğŸ§  Ù†Ù…Ø· Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ù…Ù‡ÙŠÙ…Ù†:
{get_personality_classification(positive_pct, negative_pct, speech_rate)}
        """,
        
        "speech_pattern_analysis": {
            "rate_analysis": get_arabic_speech_analysis(speech_rate),
            "consistency_rating": "Ù…ØªØ³Ù‚ ÙˆÙ…Ù†ØªØ¸Ù…" if 1.5 <= speech_rate <= 2.5 else "ØºÙŠØ± Ù…Ù†ØªØ¸Ù… ÙˆÙ‚Ø¯ ÙŠØ´ÙŠØ± Ù„ØªÙˆØªØ±",
            "psychological_indicators": get_speech_psychological_indicators(speech_rate),
            "forensic_assessment": get_forensic_speech_assessment(speech_rate)
        },
        
        "repetition_analysis": {
            "overview": f"Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙƒØ±Ø§Ø±: {repetition_data.get('repetition_level', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}",
            "psychological_meaning": repetition_data.get('psychological_analysis', []),
            "excessive_words": repetition_data.get('excessive_repetition', []),
            "interpretation": get_repetition_interpretation(repetition_data.get('repetition_ratio', 0))
        },
        
        "emotional_stability": {
            "overall_rating": get_emotional_stability_rating(negative_pct),
            "risk_assessment": get_risk_assessment(negative_pct, speech_rate),
            "recommendations": get_emotional_recommendations(negative_pct, positive_pct)
        },
        
        "psychological_profile": {
            "communication_style": get_arabic_communication_style(speech_rate, total_words),
            "dominant_traits": get_arabic_dominant_traits(positive_pct, negative_pct, speech_rate),
            "behavioral_indicators": get_behavioral_indicators(analysis_data),
            "personality_assessment": get_personality_assessment(positive_pct, negative_pct)
        },
        
        "forensic_analysis": {
            "credibility_assessment": get_credibility_assessment(analysis_data),
            "deception_indicators": get_deception_summary(analysis_data),
            "interview_suitability": calculate_interview_suitability(analysis_data),
            "risk_factors": identify_risk_factors(analysis_data)
        },
        
        "final_recommendations": {
            "psychological_development": get_psychological_recommendations(positive_pct, negative_pct, speech_rate),
            "communication_improvement": get_communication_recommendations(speech_rate, repetition_data),
            "professional_suitability": get_professional_assessment(analysis_data),
            "follow_up_suggestions": get_followup_suggestions(analysis_data)
        }
    }
    
    return report

def get_personality_classification(positive, negative, rate):
    """ØªØµÙ†ÙŠÙ Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"""
    if positive > 70 and rate > 2.0:
        return "Ø´Ø®ØµÙŠØ© Ù‚ÙŠØ§Ø¯ÙŠØ© ÙˆØ§Ø«Ù‚Ø© ÙˆÙ…ØªÙØ§Ø¦Ù„Ø©"
    elif positive > 60 and negative < 20:
        return "Ø´Ø®ØµÙŠØ© Ù…Ø³ØªÙ‚Ø±Ø© ÙˆØ¥ÙŠØ¬Ø§Ø¨ÙŠØ©"
    elif negative > 40:
        return "Ø´Ø®ØµÙŠØ© ØªØ­ØªØ§Ø¬ Ù„Ø¯Ø¹Ù… Ù†ÙØ³ÙŠ"
    elif rate < 1.5:
        return "Ø´Ø®ØµÙŠØ© Ø­Ø°Ø±Ø© ÙˆÙ…ØªØ£Ù†ÙŠØ©"
    elif rate > 2.8:
        return "Ø´Ø®ØµÙŠØ© Ù…ØªØ­Ù…Ø³Ø© ÙˆÙ‚Ø¯ ØªÙƒÙˆÙ† Ù…ØªÙˆØªØ±Ø©"
    else:
        return "Ø´Ø®ØµÙŠØ© Ù…ØªÙˆØ§Ø²Ù†Ø© ÙˆÙ…ØªÙƒÙŠÙØ©"

def get_arabic_speech_analysis(rate):
    """ØªØ­Ù„ÙŠÙ„ Ø³Ø±Ø¹Ø© Ø§Ù„ÙƒÙ„Ø§Ù… Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""
    if rate > 3.0:
        return "Ø³Ø±Ø¹Ø© Ù…ÙØ±Ø·Ø© - Ù‚Ø¯ ØªØ´ÙŠØ± Ø¥Ù„Ù‰ ØªÙˆØªØ± Ø´Ø¯ÙŠØ¯ Ø£Ùˆ Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥Ø®ÙØ§Ø¡ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª"
    elif rate > 2.5:
        return "Ø³Ø±Ø¹Ø© Ø¹Ø§Ù„ÙŠØ© - Ø´Ø®Øµ ÙˆØ§Ø«Ù‚ ÙˆÙ†Ø´Ø·ØŒ Ù„ÙƒÙ† Ù‚Ø¯ ÙŠØ­ØªØ§Ø¬ Ù„ØªÙ‡Ø¯Ø¦Ø© Ø§Ù„Ø¥ÙŠÙ‚Ø§Ø¹"
    elif rate > 2.0:
        return "Ù…Ø¹Ø¯Ù„ Ù…Ù…ØªØ§Ø² - ÙŠØ¯Ù„ Ø¹Ù„Ù‰ Ø«Ù‚Ø© ÙˆØ·Ù„Ø§Ù‚Ø© ÙÙŠ Ø§Ù„ØªØ¹Ø¨ÙŠØ±"
    elif rate > 1.5:
        return "Ù…Ø¹Ø¯Ù„ Ø·Ø¨ÙŠØ¹ÙŠ - ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ ØªØ£Ù†ÙŠ ÙˆØªÙÙƒÙŠØ± Ù‚Ø¨Ù„ Ø§Ù„ÙƒÙ„Ø§Ù…"
    else:
        return "Ø¨Ø·Ø¡ ÙÙŠ Ø§Ù„ÙƒÙ„Ø§Ù… - Ù‚Ø¯ ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ ØªØ±Ø¯Ø¯ Ø£Ùˆ Ø­Ø°Ø± Ù…ÙØ±Ø·"

def get_speech_psychological_indicators(rate):
    """Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù†ÙØ³ÙŠØ© Ù„Ø³Ø±Ø¹Ø© Ø§Ù„ÙƒÙ„Ø§Ù…"""
    indicators = []
    
    if rate > 2.8:
        indicators.extend([
            "Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ØªÙˆØªØ± ÙˆØ§Ù„Ù‚Ù„Ù‚",
            "Ø±ØºØ¨Ø© ÙÙŠ Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ø¨Ø³Ø±Ø¹Ø©", 
            "Ù‚Ø¯ ÙŠØ®ÙÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù‡Ù…Ø©"
        ])
    elif rate > 2.2:
        indicators.extend([
            "Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© Ø¨Ø§Ù„Ù†ÙØ³",
            "Ø·Ù„Ø§Ù‚Ø© ÙÙŠ Ø§Ù„ØªØ¹Ø¨ÙŠØ±",
            "Ø´Ø®ØµÙŠØ© Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©"
        ])
    elif rate < 1.5:
        indicators.extend([
            "Ø­Ø°Ø± ÙÙŠ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª",
            "ØªÙÙƒÙŠØ± Ø¹Ù…ÙŠÙ‚ Ù‚Ø¨Ù„ Ø§Ù„ÙƒÙ„Ø§Ù…",
            "Ù‚Ø¯ ÙŠØ´ÙŠØ± Ù„Ù„ØªØ±Ø¯Ø¯ Ø£Ùˆ Ø§Ù„Ø®ÙˆÙ"
        ])
    else:
        indicators.extend([
            "ØªÙˆØ§Ø²Ù† Ù†ÙØ³ÙŠ Ø¬ÙŠØ¯",
            "Ù‚Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„Ø°Ø§Øª",
            "ØªÙÙƒÙŠØ± Ù…Ù†Ø·Ù‚ÙŠ ÙˆÙ…Ù†Ø¸Ù…"
        ])
    
    return indicators

def get_forensic_speech_assessment(rate):
    """Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¬Ù†Ø§Ø¦ÙŠ Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„ÙƒÙ„Ø§Ù…"""
    if rate > 3.0:
        return "âš ï¸ Ù…Ø¤Ø´Ø± Ø®Ø·Ø± Ø¹Ø§Ù„ÙŠ - Ø³Ø±Ø¹Ø© Ù…ÙØ±Ø·Ø© Ù‚Ø¯ ØªØ®ÙÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª"
    elif rate < 1.2:
        return "âš ï¸ Ù…Ø¤Ø´Ø± Ø­Ø°Ø± - Ø¨Ø·Ø¡ Ù…ÙØ±Ø· Ù‚Ø¯ ÙŠØ´ÙŠØ± Ù„ÙƒØ°Ø¨ Ø£Ùˆ Ø¥Ø®ÙØ§Ø¡"
    elif 1.8 <= rate <= 2.3:
        return "âœ… Ù†Ù…Ø· Ø·Ø¨ÙŠØ¹ÙŠ - Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¤Ø´Ø±Ø§Øª Ø®Ø¯Ø§Ø¹ ÙˆØ§Ø¶Ø­Ø©"
    else:
        return "ğŸ“Š ÙŠØ­ØªØ§Ø¬ Ù…ØªØ§Ø¨Ø¹Ø© - Ø£Ù†Ù…Ø§Ø· ØºÙŠØ± Ù…Ø¹ØªØ§Ø¯Ø© ØªØ³ØªØ¯Ø¹ÙŠ Ø§Ù„ØªØ­Ù‚Ù‚"

def get_repetition_interpretation(ratio):
    """ØªÙØ³ÙŠØ± Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙƒØ±Ø§Ø± Ù†ÙØ³ÙŠØ§Ù‹"""
    if ratio > 15:
        return "ØªÙƒØ±Ø§Ø± Ù…ÙØ±Ø· - Ù‚Ø¯ ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ø§Ù„Ù‚Ù„Ù‚ Ø£Ùˆ Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø¥Ù‚Ù†Ø§Ø¹ Ø¨Ø§Ù„Ù‚ÙˆØ©"
    elif ratio > 10:
        return "ØªÙƒØ±Ø§Ø± Ø¹Ø§Ù„ÙŠ - Ù‚Ø¯ ÙŠØ¯Ù„ Ø¹Ù„Ù‰ Ø§Ù„ØªØ£ÙƒÙŠØ¯ Ø£Ùˆ Ø¹Ø¯Ù… Ø§Ù„Ø«Ù‚Ø© ÙÙŠ Ø§Ù„Ø±Ø³Ø§Ù„Ø©"
    elif ratio > 5:
        return "ØªÙƒØ±Ø§Ø± Ø·Ø¨ÙŠØ¹ÙŠ - ÙŠØ³ØªØ®Ø¯Ù… Ù„Ù„ØªØ£ÙƒÙŠØ¯ ÙˆØ§Ù„ÙˆØ¶ÙˆØ­"
    else:
        return "ØªÙƒØ±Ø§Ø± Ù…Ù†Ø®ÙØ¶ - ÙƒÙ„Ø§Ù… Ù…ØªÙ†ÙˆØ¹ ÙˆÙ…ØªØ¯ÙÙ‚"

def get_emotional_stability_rating(negative_pct):
    """ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„Ø¹Ø§Ø·ÙÙŠ"""
    if negative_pct < 15:
        return "Ù…Ø³ØªÙ‚Ø± Ø¹Ø§Ø·ÙÙŠØ§Ù‹ Ø¨Ø¯Ø±Ø¬Ø© Ù…Ù…ØªØ§Ø²Ø©"
    elif negative_pct < 25:
        return "Ù…Ø³ØªÙ‚Ø± Ø¹Ø§Ø·ÙÙŠØ§Ù‹ Ø¨Ø¯Ø±Ø¬Ø© Ø¬ÙŠØ¯Ø©"
    elif negative_pct < 40:
        return "Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø¹Ø§Ø·ÙÙŠ Ù…ØªÙˆØ³Ø·"
    else:
        return "ÙŠØ­ØªØ§Ø¬ Ù„Ø¯Ø¹Ù… ÙÙŠ Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„Ø¹Ø§Ø·ÙÙŠ"

def get_risk_assessment(negative_pct, speech_rate):
    """ØªÙ‚ÙŠÙŠÙ… Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ø®Ø§Ø·Ø±"""
    risk_score = 0
    
    if negative_pct > 50:
        risk_score += 3
    elif negative_pct > 30:
        risk_score += 2
    elif negative_pct > 15:
        risk_score += 1
    
    if speech_rate > 3.5 or speech_rate < 1.0:
        risk_score += 2
    elif speech_rate > 3.0 or speech_rate < 1.2:
        risk_score += 1
    
    if risk_score >= 4:
        return "Ù…Ø®Ø§Ø·Ø± Ø¹Ø§Ù„ÙŠØ© - ÙŠØ­ØªØ§Ø¬ ØªØ¯Ø®Ù„ ÙÙˆØ±ÙŠ"
    elif risk_score >= 2:
        return "Ù…Ø®Ø§Ø·Ø± Ù…ØªÙˆØ³Ø·Ø© - ÙŠØ­ØªØ§Ø¬ Ù…ØªØ§Ø¨Ø¹Ø©"
    else:
        return "Ù…Ø®Ø§Ø·Ø± Ù…Ù†Ø®ÙØ¶Ø© - Ø­Ø§Ù„Ø© Ø·Ø¨ÙŠØ¹ÙŠØ©"

def get_emotional_recommendations(negative_pct, positive_pct):
    """ØªÙˆØµÙŠØ§Øª Ù„Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„Ø¹Ø§Ø·ÙÙŠ"""
    recommendations = []
    
    if negative_pct > 40:
        recommendations.extend([
            "Ø¶Ø±ÙˆØ±Ø© Ø§Ø³ØªØ´Ø§Ø±Ø© Ù†ÙØ³ÙŠØ© Ù…ØªØ®ØµØµØ©",
            "ØªØ·Ø¨ÙŠÙ‚ ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø§Ø³ØªØ±Ø®Ø§Ø¡ ÙˆØ§Ù„ØªØ£Ù…Ù„",
            "Ù…Ù…Ø§Ø±Ø³Ø© Ø§Ù„Ø±ÙŠØ§Ø¶Ø© ÙˆØ§Ù„Ø£Ù†Ø´Ø·Ø© Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©"
        ])
    elif negative_pct > 25:
        recommendations.extend([
            "ØªØ·ÙˆÙŠØ± Ø¢Ù„ÙŠØ§Øª Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¶ØºØ·",
            "ØªØ­Ø³ÙŠÙ† Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠ"
        ])
    
    if positive_pct > 70:
        recommendations.append("Ø§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù† Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ© ÙÙŠ Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©")
    
    return recommendations

def get_arabic_communication_style(speech_rate, total_words):
    """ØªØ­Ù„ÙŠÙ„ Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„ØªÙˆØ§ØµÙ„"""
    if speech_rate > 2.5 and total_words > 100:
        return "Ù…ØªØ­Ø¯Ø« Ø·Ù„Ù‚ ÙˆÙˆØ§Ø«Ù‚ - ÙŠÙØ¶Ù„ Ø§Ù„ØªÙØµÙŠÙ„"
    elif speech_rate > 2.2:
        return "Ù…ØªØ­Ø¯Ø« Ù†Ø´Ø· - ÙŠØ­Ø¨ Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ©"
    elif speech_rate < 1.8:
        return "Ù…ØªØ­Ø¯Ø« Ù…ØªØ£Ù†ÙŠ - ÙŠÙÙƒØ± Ù‚Ø¨Ù„ Ø§Ù„ÙƒÙ„Ø§Ù…"
    else:
        return "Ù…ØªØ­Ø¯Ø« Ù…ØªÙˆØ§Ø²Ù† - Ø£Ø³Ù„ÙˆØ¨ Ù…Ù†Ø§Ø³Ø¨"

def get_arabic_dominant_traits(positive_pct, negative_pct, speech_rate):
    """Ø§Ù„ØµÙØ§Øª Ø§Ù„Ù…Ù‡ÙŠÙ…Ù†Ø© ÙÙŠ Ø§Ù„Ø´Ø®ØµÙŠØ©"""
    traits = []
    
    if positive_pct > 60:
        traits.append("Ù…ØªÙØ§Ø¦Ù„ ÙˆØ¥ÙŠØ¬Ø§Ø¨ÙŠ")
    if speech_rate > 2.3:
        traits.append("Ù†Ø´Ø· ÙˆØ­ÙŠÙˆÙŠ")
    if negative_pct < 20:
        traits.append("Ù…Ø³ØªÙ‚Ø± Ù†ÙØ³ÙŠØ§Ù‹")
    if speech_rate < 1.8:
        traits.append("Ø­Ø°Ø± ÙˆÙ…ØªØ£Ù†ÙŠ")
    if speech_rate > 2.8:
        traits.append("Ù…ØªØ­Ù…Ø³ ÙˆÙ‚Ø¯ ÙŠÙƒÙˆÙ† Ù…ØªÙˆØªØ±")
    
    return traits if traits else ["Ø´Ø®ØµÙŠØ© Ù…ØªÙˆØ§Ø²Ù†Ø©"]

def get_behavioral_indicators(analysis_data):
    """Ù…Ø¤Ø´Ø±Ø§Øª Ø³Ù„ÙˆÙƒÙŠØ©"""
    indicators = []
    speech_rate = analysis_data.get('speech_rate_wps', 0)
    sentiment = analysis_data.get('sentiment', {})
    
    if speech_rate > 3.0:
        indicators.append("Ù‚Ø¯ ÙŠÙƒÙˆÙ† ØªØ­Øª Ø¶ØºØ· Ø£Ùˆ ØªÙˆØªØ±")
    if float(sentiment.get('negative', '0%').replace('%', '')) > 30:
        indicators.append("ÙŠØ¸Ù‡Ø± Ø¹Ù„Ø§Ù…Ø§Øª Ø¹Ø¯Ù… Ø§Ù„Ø±Ø§Ø­Ø©")
    
    return indicators

def get_personality_assessment(positive_pct, negative_pct):
    """ØªÙ‚ÙŠÙŠÙ… Ø´Ø§Ù…Ù„ Ù„Ù„Ø´Ø®ØµÙŠØ©"""
    if positive_pct > 70 and negative_pct < 15:
        return "Ø´Ø®ØµÙŠØ© Ù‚ÙˆÙŠØ© ÙˆÙ…ØªÙØ§Ø¦Ù„Ø© - Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„Ø£Ø¯ÙˆØ§Ø± Ø§Ù„Ù‚ÙŠØ§Ø¯ÙŠØ©"
    elif positive_pct > 50 and negative_pct < 25:
        return "Ø´Ø®ØµÙŠØ© Ù…ØªÙˆØ§Ø²Ù†Ø© - Ù‚Ø§Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªÙƒÙŠÙ"
    elif negative_pct > 40:
        return "ØªØ­ØªØ§Ø¬ Ù„ØªØ·ÙˆÙŠØ± Ø§Ù„Ø¬Ø§Ù†Ø¨ Ø§Ù„Ù†ÙØ³ÙŠ ÙˆØ§Ù„Ø¹Ø§Ø·ÙÙŠ"
    else:
        return "Ø´Ø®ØµÙŠØ© Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆÙ…Ù‚Ø¨ÙˆÙ„Ø©"

def get_credibility_assessment(analysis_data):
    """ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…ØµØ¯Ø§Ù‚ÙŠØ©"""
    speech_rate = analysis_data.get('speech_rate_wps', 0)
    
    if 1.8 <= speech_rate <= 2.5:
        return "Ù…ØµØ¯Ø§Ù‚ÙŠØ© Ø¹Ø§Ù„ÙŠØ© - Ø£Ù†Ù…Ø§Ø· ÙƒÙ„Ø§Ù… Ø·Ø¨ÙŠØ¹ÙŠØ©"
    elif speech_rate > 3.0 or speech_rate < 1.2:
        return "ØªØ­ØªØ§Ø¬ Ù„Ù„ØªØ­Ù‚Ù‚ - Ø£Ù†Ù…Ø§Ø· ØºÙŠØ± Ø¹Ø§Ø¯ÙŠØ©"
    else:
        return "Ù…ØµØ¯Ø§Ù‚ÙŠØ© Ù…ØªÙˆØ³Ø·Ø©"

def get_deception_summary(analysis_data):
    """Ù…Ù„Ø®Øµ Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø®Ø¯Ø§Ø¹"""
    deception_indicators = analysis_data.get('deception_indicators', {})
    risk_level = deception_indicators.get('risk_level', 'Ù…Ù†Ø®ÙØ¶')
    
    return f"Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ø®Ø§Ø·Ø±: {risk_level}"

def calculate_interview_suitability(analysis_data):
    """Ø­Ø³Ø§Ø¨ Ù…Ù†Ø§Ø³Ø¨Ø© Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø©"""
    speech_rate = analysis_data.get('speech_rate_wps', 0)
    sentiment = analysis_data.get('sentiment', {})
    positive_pct = float(sentiment.get('positive', '0%').replace('%', ''))
    
    score = 0
    if 1.5 <= speech_rate <= 2.8:
        score += 3
    if positive_pct > 50:
        score += 2
    
    if score >= 4:
        return "Ù…Ù…ØªØ§Ø² Ù„Ù„Ù…Ù‚Ø§Ø¨Ù„Ø§Øª"
    elif score >= 2:
        return "Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ù…Ù‚Ø§Ø¨Ù„Ø§Øª"
    else:
        return "ÙŠØ­ØªØ§Ø¬ ØªØ­Ø¶ÙŠØ± Ø¥Ø¶Ø§ÙÙŠ"

def identify_risk_factors(analysis_data):
    """ØªØ­Ø¯ÙŠØ¯ Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„Ù…Ø®Ø§Ø·Ø±"""
    risks = []
    speech_rate = analysis_data.get('speech_rate_wps', 0)
    sentiment = analysis_data.get('sentiment', {})
    negative_pct = float(sentiment.get('negative', '0%').replace('%', ''))
    
    if speech_rate > 3.5:
        risks.append("Ø³Ø±Ø¹Ø© ÙƒÙ„Ø§Ù… Ù…ÙØ±Ø·Ø©")
    if negative_pct > 50:
        risks.append("Ù…Ø´Ø§Ø¹Ø± Ø³Ù„Ø¨ÙŠØ© Ø¹Ø§Ù„ÙŠØ©")
    
    return risks if risks else ["Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø®Ø§Ø·Ø± ÙˆØ§Ø¶Ø­Ø©"]

def get_psychological_recommendations(positive_pct, negative_pct, speech_rate):
    """ØªÙˆØµÙŠØ§Øª Ø§Ù„ØªØ·ÙˆÙŠØ± Ø§Ù„Ù†ÙØ³ÙŠ"""
    recommendations = []
    
    if negative_pct > 30:
        recommendations.append("Ø§Ù„Ø¹Ù…Ù„ Ø¹Ù„Ù‰ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø²Ø§Ø¬ÙŠØ©")
    if speech_rate > 3.0:
        recommendations.append("ØªØ·ÙˆÙŠØ± Ù…Ù‡Ø§Ø±Ø§Øª Ø§Ù„Ù‡Ø¯ÙˆØ¡ ÙˆØ§Ù„ØªØ±ÙƒÙŠØ²")
    if positive_pct < 40:
        recommendations.append("ØªØ¹Ø²ÙŠØ² Ø§Ù„Ø«Ù‚Ø© Ø¨Ø§Ù„Ù†ÙØ³")
    
    return recommendations

def get_communication_recommendations(speech_rate, repetition_data):
    """ØªÙˆØµÙŠØ§Øª ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙˆØ§ØµÙ„"""
    recommendations = []
    
    if speech_rate > 2.8:
        recommendations.append("ØªÙ‚Ù„ÙŠÙ„ Ø³Ø±Ø¹Ø© Ø§Ù„ÙƒÙ„Ø§Ù… Ù„Ù„ÙˆØ¶ÙˆØ­")
    if repetition_data.get('repetition_ratio', 0) > 12:
        recommendations.append("ØªÙ‚Ù„ÙŠÙ„ ØªÙƒØ±Ø§Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª")
    
    return recommendations

def get_professional_assessment(analysis_data):
    """Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ù‡Ù†ÙŠ"""
    speech_rate = analysis_data.get('speech_rate_wps', 0)
    sentiment = analysis_data.get('sentiment', {})
    positive_pct = float(sentiment.get('positive', '0%').replace('%', ''))
    
    if positive_pct > 60 and 1.8 <= speech_rate <= 2.5:
        return "Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ø£Ø¯ÙˆØ§Ø± Ø§Ù„Ù…Ù‡Ù†ÙŠØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"
    elif positive_pct > 40:
        return "Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ø£Ø¯ÙˆØ§Ø± Ø§Ù„Ù…Ù‡Ù†ÙŠØ© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©"
    else:
        return "ÙŠØ­ØªØ§Ø¬ ØªØ·ÙˆÙŠØ± Ù…Ù‡Ù†ÙŠ"

def evaluate_response_quality(text):
    """ØªÙ‚ÙŠÙŠÙ… Ø¬ÙˆØ¯Ø© Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© - Ø§Ù„Ø¹Ù…Ù‚ ÙˆØ§Ù„Ø®ØµÙˆØµÙŠØ©"""
    score = 0

    # ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¹Ù…Ù‚
    depth_keywords = ["ØªØ­Ù„ÙŠÙ„", "Ø£Ø«Ø¨Øª", "Ù†Ø§Ù‚Ø´", "Ø§Ø³ØªÙ†ØªØ¬", "ÙŠÙØ³Ø±", "Ù…Ù‚Ø§Ø±Ù†Ø©", "ØªÙ‚ÙŠÙŠÙ…", "Ø£Ø³Ø¨Ø§Ø¨", "Ù†ØªØ§Ø¦Ø¬", "Ø£Ø¯Ù„Ø©"]
    
    # ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø®ØµÙˆØµÙŠØ©
    specificity_keywords = ["ØªØ­Ø¯ÙŠØ¯Ù‹Ø§", "Ø¨Ø§Ù„Ø¶Ø¨Ø·", "ØªØ´ÙŠØ± Ø§Ù„Ø£Ø¨Ø­Ø§Ø«", "ÙÙŠ Ø¹Ø§Ù…", "Ø­Ø³Ø¨ Ø¯Ø±Ø§Ø³Ø©",
                           "ÙÙŠ Ù…Ø¬Ø§Ù„", "ÙˆÙÙ‚Ù‹Ø§ Ù„Ù€", "ØªØ¬Ø±Ø¨Ø©", "Ø§Ø³Ù… Ø¨Ø§Ø­Ø«", "Ù†ÙˆØ¹ Ù…Ø¹ÙŠÙ†", "Ù…ÙƒØ§Ù† Ù…Ø¹ÙŠÙ†"]

    # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¹Ù…Ù‚
    if any(word in text for word in depth_keywords):
        score += 0.5

    # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø®ØµÙˆØµÙŠØ©
    if any(word in text for word in specificity_keywords):
        score += 0.5

    # ØªÙ‚ÙŠÙŠÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
    # ÙˆØ¬ÙˆØ¯ Ø£Ù…Ø«Ù„Ø©
    if "Ù…Ø«Ø§Ù„" in text or "Ù…Ø«Ù„" in text:
        score += 0.15

    # Ø§Ù„Ø·ÙˆÙ„ ÙˆØ¹Ø¯Ø¯ Ø§Ù„Ø¬Ù…Ù„
    if len(text.split('.')) > 4:
        score += 0.15

    # ÙˆØ¬ÙˆØ¯ ÙƒÙ„Ù…Ø§Øª ØªØ­Ù„ÙŠÙ„ÙŠØ©
    if any(w in text for w in ["ØªØ­Ù„ÙŠÙ„", "Ø£Ø«Ø¨Øª", "Ø§Ø³ØªÙ†ØªØ¬", "Ù†Ø§Ù‚Ø´"]):
        score += 0.2

    # Ø§Ù„ØªØ®ØµØµ Ø£Ùˆ Ø°ÙƒØ± Ù…Ø¬Ø§Ù„ Ù…Ø­Ø¯Ø¯
    if any(w in text for w in ["Ù…Ø¬Ø§Ù„", "Ù…ÙˆØ¶ÙˆØ¹", "ØªØ®ØµØµ"]):
        score += 0.15

    # Ø¯Ù‚Ø©/Ø®ØµÙˆØµÙŠØ©
    if any(w in text for w in ["Ø¨Ø§Ù„Ø¶Ø¨Ø·", "ØªØ­Ø¯ÙŠØ¯Ù‹Ø§", "Ø¹Ù„Ù‰ ÙˆØ¬Ù‡ Ø§Ù„ØªØ­Ø¯ÙŠØ¯", "Ø¨Ø§Ù„ØªÙØµÙŠÙ„"]):
        score += 0.15

    # ØªØ±Ø§Ø¨Ø· ÙˆÙˆØ¶ÙˆØ­
    if any(w in text for w in ["Ù„Ø°Ù„Ùƒ", "Ø¨Ø§Ù„ØªØ§Ù„ÙŠ", "Ø¹Ù„Ø§ÙˆØ© Ø¹Ù„Ù‰ Ø°Ù„Ùƒ", "Ù…Ù† Ù†Ø§Ø­ÙŠØ© Ø£Ø®Ø±Ù‰"]):
        score += 0.2

    final_score = round(min(score, 1.0), 2)
    
    if final_score >= 0.8:
        level = "Ù…Ù…ØªØ§Ø²"
    elif final_score >= 0.6:
        level = "Ø¬ÙŠØ¯ Ø¬Ø¯Ø§Ù‹"  
    elif final_score >= 0.4:
        level = "Ø¬ÙŠØ¯"
    else:
        level = "ÙŠØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ†"
        
    return {
        "score": final_score,
        "level": level,
        "depth_indicators": [word for word in depth_keywords if word in text],
        "specificity_indicators": [word for word in specificity_keywords if word in text]
    }

def analyze_hesitation_patterns(text):
    """ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªØ±Ø¯Ø¯ ÙˆÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø­Ø´Ùˆ"""
    import re
    from collections import Counter
    
    # ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø­Ø´Ùˆ ÙˆØ§Ù„ØªØ±Ø¯Ø¯ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© ÙÙŠ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
    filler_words = ["ÙŠØ¹Ù†ÙŠ", "Ù‡Ùˆ", "Ø¨Ø³", "ÙƒØ¯Ù‡", "Ø§Ù‡", "Ø§ÙŠÙˆØ©", "Ø·ÙŠØ¨", "Ø®Ù„Ø§Øµ", "Ù…Ø§ Ù‡Ùˆ", "ÙŠØ§ Ø¥Ù…Ø§"]
    hesitation_patterns = ["Ù…Ù…Ù…", "Ø§Ø§Ø§Ù‡", "Ù‡Ù…Ù…Ù…", "Ø¥ÙŠÙ‡", "Ø§Ø²Ø§ÙŠ", "ÙŠØ¹Ù†ÙŠ ÙƒØ¯Ù‡"]
    
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ
    text_clean = re.sub(r'[^\w\s]', '', text.lower())
    words = text_clean.split()
    
    # Ø­Ø³Ø§Ø¨ ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø­Ø´Ùˆ
    filler_count = {}
    total_fillers = 0
    for word in filler_words:
        count = words.count(word)
        if count > 0:
            filler_count[word] = count
            total_fillers += count
    
    # Ø­Ø³Ø§Ø¨ Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªØ±Ø¯Ø¯
    hesitation_count = {}
    total_hesitations = 0
    for pattern in hesitation_patterns:
        count = text.lower().count(pattern)
        if count > 0:
            hesitation_count[pattern] = count
            total_hesitations += count
    
    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ø³Ø¨
    total_words = len(words)
    filler_ratio = (total_fillers / total_words * 100) if total_words > 0 else 0
    hesitation_ratio = (total_hesitations / total_words * 100) if total_words > 0 else 0
    
    # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø³ØªÙˆÙ‰
    if filler_ratio > 15:
        fluency_level = "Ù…ØªØ±Ø¯Ø¯ Ø¬Ø¯Ø§Ù‹"
    elif filler_ratio > 8:
        fluency_level = "Ù…ØªØ±Ø¯Ø¯"
    elif filler_ratio > 3:
        fluency_level = "Ù…ØªØ±Ø¯Ø¯ Ù‚Ù„ÙŠÙ„Ø§Ù‹"
    else:
        fluency_level = "Ø·Ù„Ù‚"
    
    return {
        "filler_words": filler_count,
        "hesitation_patterns": hesitation_count,
        "filler_ratio": round(filler_ratio, 2),
        "hesitation_ratio": round(hesitation_ratio, 2),
        "fluency_level": fluency_level,
        "total_fillers": total_fillers,
        "total_hesitations": total_hesitations
    }

def detect_soft_skills(text):
    """ÙƒØ´Ù Ø§Ù„Ù…Ù‡Ø§Ø±Ø§Øª Ø§Ù„Ù†Ø§Ø¹Ù…Ø©"""
    
    soft_skills_keywords = {
        "Ø§Ù„ØªÙˆØ§ØµÙ„": ["ØªÙˆØ§ØµÙ„", "Ø¥Ù‚Ù†Ø§Ø¹", "Ø§Ø³ØªÙ…Ø§Ø¹", "Ø¹Ø±Ø¶", "Ø´Ø±Ø­", "Ù†Ù‚Ø§Ø´", "Ø­ÙˆØ§Ø±", "ØªÙˆØ¶ÙŠØ­"],
        "Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©": ["Ù‚ÙŠØ§Ø¯Ø©", "ØªÙˆØ¬ÙŠÙ‡", "Ø¥Ù„Ù‡Ø§Ù…", "Ø¥Ø¯Ø§Ø±Ø©", "ØªØ­ÙÙŠØ²", "Ø±ÙŠØ§Ø¯Ø©", "Ù…Ø³Ø¤ÙˆÙ„ÙŠØ©"],
        "Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ø¬Ù…Ø§Ø¹ÙŠ": ["ÙØ±ÙŠÙ‚", "ØªØ¹Ø§ÙˆÙ†", "ØªÙ†Ø³ÙŠÙ‚", "Ø¬Ù…Ø§Ø¹ÙŠ", "Ù…Ø´ØªØ±Ùƒ", "Ø´Ø±Ø§ÙƒØ©"],
        "Ø­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø§Øª": ["Ø­Ù„", "Ù…Ø´ÙƒÙ„Ø©", "ØªØ­Ø¯ÙŠ", "ØªØ­Ù„ÙŠÙ„", "Ø­Ù„ÙˆÙ„", "Ù…Ø¹Ø§Ù„Ø¬Ø©"],
        "Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ù†Ù‚Ø¯ÙŠ": ["ØªÙÙƒÙŠØ±", "Ù†Ù‚Ø¯", "Ù…Ù†Ø·Ù‚", "Ø£Ø¯Ù„Ø©", "ØªÙ‚ÙŠÙŠÙ…", "Ø§Ø³ØªÙ†ØªØ§Ø¬"],
        "Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„ÙˆÙ‚Øª": ["ØªÙ†Ø¸ÙŠÙ…", "ÙˆÙ‚Øª", "Ø¬Ø¯ÙˆÙ„", "Ø§Ù„ØªØ²Ø§Ù…", "Ø£ÙˆÙ„ÙˆÙŠØ©", "ØªØ®Ø·ÙŠØ·"],
        "Ø§Ù„Ù…Ø±ÙˆÙ†Ø©": ["ØªØ£Ù‚Ù„Ù…", "Ù…Ø±ÙˆÙ†Ø©", "Ø¸Ø±ÙˆÙ", "ØªØºÙŠÙŠØ±", "ØªÙƒÙŠÙ", "Ø§Ø³ØªØ¬Ø§Ø¨Ø©"],
        "Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹": ["Ø§Ø¨ØªÙƒØ§Ø±", "Ø¥Ø¨Ø¯Ø§Ø¹", "Ø£ÙÙƒØ§Ø±", "Ø®Ø§Ø±Ø¬ Ø§Ù„ØµÙ†Ø¯ÙˆÙ‚", "Ø®Ù„Ø§Ù‚Ø©", "Ù…Ø¨ØªÙƒØ±"]
    }
    
    found_skills = {}
    skill_scores = {}
    
    for skill, keywords in soft_skills_keywords.items():
        skill_keywords_found = []
        for keyword in keywords:
            if keyword in text:
                skill_keywords_found.append(keyword)
        
        if skill_keywords_found:
            found_skills[skill] = skill_keywords_found
            # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„Ù‚ÙˆØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙƒØªØ´ÙØ©
            skill_scores[skill] = len(skill_keywords_found) / len(keywords)
    
    return {
        "detected_skills": found_skills,
        "skill_scores": skill_scores,
        "skills_count": len(found_skills)
    }

def measure_engagement_level(text):
    """Ù‚ÙŠØ§Ø³ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ© ÙˆØ§Ù„Ø­Ù…Ø§Ø³"""
    
    high_engagement_keywords = ['Ù„Ù…Ø§Ø°Ø§', 'ÙƒÙŠÙ', 'Ù‡Ù„ ÙŠÙ…ÙƒÙ†', 'Ø£Ø¹ØªÙ‚Ø¯', 'Ø¨Ø±Ø£ÙŠÙŠ', 'Ø§Ù‚ØªØ±Ø§Ø­', 'Ø³Ø¤Ø§Ù„', 'Ù†Ù‚Ø§Ø´', 'Ù…Ø§ Ø±Ø£ÙŠÙƒÙ…']
    medium_engagement_keywords = ['Ø¬Ù…ÙŠÙ„', 'Ù…ÙÙŠØ¯', 'Ø´ÙƒØ±Ø§', 'Ù…Ù…ØªØ§Ø²', 'Ø±Ø§Ø¦Ø¹', 'Ø£Ø­Ø¨Ø¨Øª', 'Ù…ÙˆØ§ÙÙ‚']
    low_engagement_keywords = ['.', '...', 'Ù†Ø¹Ù…', 'Ù„Ø§']
    
    text_lower = text.lower()
    
    high_score = sum(1 for word in high_engagement_keywords if word in text_lower)
    medium_score = sum(1 for word in medium_engagement_keywords if word in text_lower)
    low_score = sum(1 for word in low_engagement_keywords if word in text_lower)
    
    total_indicators = high_score + medium_score + low_score
    
    if high_score > 0:
        level = "Ù…Ø´Ø§Ø±ÙƒØ© Ø¹Ø§Ù„ÙŠØ©"
        score = 0.8 + (high_score * 0.05)
    elif medium_score > 0:
        level = "Ù…Ø´Ø§Ø±ÙƒØ© Ù…ØªÙˆØ³Ø·Ø©"
        score = 0.5 + (medium_score * 0.05)
    else:
        level = "Ù…Ø´Ø§Ø±ÙƒØ© Ù…Ù†Ø®ÙØ¶Ø©"
        score = 0.2
    
    return {
        "engagement_level": level,
        "engagement_score": min(round(score, 2), 1.0),
        "high_engagement_count": high_score,
        "medium_engagement_count": medium_score,
        "low_engagement_count": low_score,
        "engagement_indicators": {
            "high": [word for word in high_engagement_keywords if word in text_lower],
            "medium": [word for word in medium_engagement_keywords if word in text_lower],
            "low": [word for word in low_engagement_keywords if word in text_lower]
        }
    }

def get_followup_suggestions(analysis_data):
    """Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø©"""
    return [
        "Ø¥Ø¹Ø§Ø¯Ø© ØªÙ‚ÙŠÙŠÙ… Ø¨Ø¹Ø¯ 3 Ø£Ø´Ù‡Ø±",
        "ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©",
        "Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„ØªØ­Ø³Ù† ÙÙŠ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª"
    ]

def get_forensic_speech_analysis(rate):
    """ØªØ­Ù„ÙŠÙ„ Ø¬Ù†Ø§Ø¦ÙŠ Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„ÙƒÙ„Ø§Ù…"""
    if rate > 3.0:
        return "Ø³Ø±Ø¹Ø© Ù…ÙØ±Ø·Ø© - Ù‚Ø¯ ØªØ´ÙŠØ± Ø¥Ù„Ù‰ ØªÙˆØªØ± Ø£Ùˆ Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥Ø®ÙØ§Ø¡ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª"
    elif rate > 2.5:
        return "Ø³Ø±Ø¹Ø© Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¹Ø§Ù„ÙŠØ© - Ø´Ø®Øµ ÙˆØ§Ø«Ù‚ ÙˆÙ…Ø±ØªØ§Ø­"
    elif rate > 2.0:
        return "Ù…Ø¹Ø¯Ù„ Ø·Ø¨ÙŠØ¹ÙŠ - Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¤Ø´Ø±Ø§Øª ØºÙŠØ± Ø·Ø¨ÙŠØ¹ÙŠØ©"
    elif rate > 1.5:
        return "Ù…Ø¹Ø¯Ù„ Ù…Ù†Ø®ÙØ¶ - Ù‚Ø¯ ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ø­Ø°Ø± Ø£Ùˆ ØªÙÙƒÙŠØ± Ø¹Ù…ÙŠÙ‚"
    else:
        return "Ø¨Ø·Ø¡ Ù…ÙØ±Ø· - Ù‚Ø¯ ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ ØªØ±Ø¯Ø¯ Ø£Ùˆ Ø¥Ø®ÙØ§Ø¡ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª"

def get_communication_style(rate, words):
    """ØªØ­Ø¯ÙŠØ¯ Ù†Ù…Ø· Ø§Ù„ØªÙˆØ§ØµÙ„"""
    if rate > 2.3 and words > 150:
        return "Ù…ØªØ­Ø¯Ø« Ø¨Ø§Ø±Ø¹ ÙˆÙˆØ§Ø«Ù‚"
    elif rate < 1.7 and words < 100:
        return "Ù…ØªØ­Ø¯Ø« Ø­Ø°Ø± ÙˆÙ…Ø®ØªØµØ±"
    elif words > 200:
        return "Ù…ØªØ­Ø¯Ø« Ù…ÙØµÙ„ ÙˆÙ…Ø¹Ø¨Ø±"
    else:
        return "Ù…ØªØ­Ø¯Ø« Ù…ØªÙˆØ§Ø²Ù†"

def get_dominant_traits(positive, negative, rate):
    """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø³Ù…Ø§Øª Ø§Ù„Ù…Ù‡ÙŠÙ…Ù†Ø©"""
    traits = []
    
    if positive > 60:
        traits.append("Ù…ØªÙØ§Ø¦Ù„")
    if negative < 20:
        traits.append("Ù…Ø³ØªÙ‚Ø± Ø¹Ø§Ø·ÙÙŠØ§Ù‹")
    if rate > 2.2:
        traits.append("Ù†Ø´Ø· ÙˆÙ…ØªÙØ§Ø¹Ù„")
    if rate < 1.8:
        traits.append("Ù…ØªØ£Ù†ÙŠ ÙˆÙ…Ø¯Ø±ÙˆØ³")
    
    return traits if traits else ["Ù…ØªÙˆØ§Ø²Ù†"]

def get_psychological_recommendations(positive, negative, rate):
    """ØªÙˆØµÙŠØ§Øª Ù†ÙØ³ÙŠØ© Ù„Ù„ØªØ·ÙˆÙŠØ±"""
    recommendations = []
    
    if negative > 30:
        recommendations.append("Ø§Ù„Ø¹Ù…Ù„ Ø¹Ù„Ù‰ ØªÙ‚Ù†ÙŠØ§Øª Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¶ØºÙˆØ· ÙˆØ§Ù„Ø§Ø³ØªØ±Ø®Ø§Ø¡")
    if rate > 2.8:
        recommendations.append("Ù…Ù…Ø§Ø±Ø³Ø© Ø§Ù„ØªÙ†ÙØ³ Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù„ØªÙ‡Ø¯Ø¦Ø© Ù…Ø¹Ø¯Ù„ Ø§Ù„ÙƒÙ„Ø§Ù…")
    if rate < 1.5:
        recommendations.append("ØªØ·ÙˆÙŠØ± Ø§Ù„Ø«Ù‚Ø© Ø¨Ø§Ù„Ù†ÙØ³ Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨")
    if positive < 40:
        recommendations.append("Ø§Ù„Ø¹Ù…Ù„ Ø¹Ù„Ù‰ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ø¸Ø±Ø© Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ© Ù„Ù„Ø­ÙŠØ§Ø©")
    
    return recommendations if recommendations else ["Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø­Ø§Ù„ÙŠ Ø§Ù„Ø¬ÙŠØ¯"]

def calculate_interview_score(positive, negative, rate):
    """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ù…Ù„Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø©"""
    score = 0
    
    # Ø¯Ø±Ø¬Ø© Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ© (40 Ù†Ù‚Ø·Ø©)
    score += min(40, positive * 0.67)
    
    # ØªØ®ØµÙ… Ù„Ù„Ø³Ù„Ø¨ÙŠØ© (20 Ù†Ù‚Ø·Ø©)
    score += max(0, 20 - negative * 0.5)
    
    # Ø¯Ø±Ø¬Ø© Ù…Ø¹Ø¯Ù„ Ø§Ù„ÙƒÙ„Ø§Ù… (40 Ù†Ù‚Ø·Ø©) 
    if 1.5 <= rate <= 2.5:
        score += 40
    elif 1.2 <= rate <= 2.8:
        score += 30
    else:
        score += 20
        
    return min(100, round(score))

def get_interview_strengths(positive, rate):
    """Ù†Ù‚Ø§Ø· Ø§Ù„Ù‚ÙˆØ© ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø©"""
    strengths = []
    
    if positive > 60:
        strengths.append("Ù…ÙˆÙ‚Ù Ø¥ÙŠØ¬Ø§Ø¨ÙŠ ÙˆÙ…ØªÙØ§Ø¦Ù„")
    if 1.8 <= rate <= 2.3:
        strengths.append("Ù…Ø¹Ø¯Ù„ ÙƒÙ„Ø§Ù… Ù…Ø«Ø§Ù„ÙŠ Ù„Ù„ØªÙˆØ§ØµÙ„")
    if rate > 2.0:
        strengths.append("Ø«Ù‚Ø© ÙˆØ§Ø¶Ø­Ø© ÙÙŠ Ø§Ù„ØªØ¹Ø¨ÙŠØ±")
        
    return strengths if strengths else ["Ù‚Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ø¨ÙŠØ± Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©"]

def get_improvement_areas(negative, rate):
    """Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ†"""
    areas = []
    
    if negative > 25:
        areas.append("Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø³Ù„Ø¨ÙŠØ© ÙˆØ§Ù„ØªÙˆØªØ±")
    if rate > 2.7:
        areas.append("ØªÙ‡Ø¯Ø¦Ø© Ù…Ø¹Ø¯Ù„ Ø§Ù„ÙƒÙ„Ø§Ù… Ù„Ù„ÙˆØ¶ÙˆØ­")
    if rate < 1.5:
        areas.append("Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø«Ù‚Ø© ÙˆØ§Ù„Ø­ÙŠÙˆÙŠØ© ÙÙŠ Ø§Ù„ØªØ¹Ø¨ÙŠØ±")
        
    return areas if areas else ["Ù…ÙˆØ§ØµÙ„Ø© Ø§Ù„ØªØ·ÙˆÙŠØ± Ø§Ù„Ù…Ø³ØªÙ…Ø±"]

def analyze_all(transcript):
    """Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©"""
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ù†Øµ Ù…ÙØ±Ø¯ Ù„Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
    combined_text = " ".join([item['text'] for item in transcript])
    
    basic_analysis = {
        "sentiment": analyze_sentiment(transcript),
        "total_words": count_total_words(transcript),
        "frequent_words": get_frequent_words(transcript),
        "speech_rate_wps": calculate_overall_speech_rate(transcript),
        "sensitive_words": detect_sensitive_words(transcript),
        "translation": translate_to_english(transcript)
    }
    
    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
    basic_analysis["psychological_analysis"] = analyze_psychological_patterns(transcript)
    basic_analysis["deception_analysis"] = analyze_deception_indicators(transcript)
    basic_analysis["personality_traits"] = analyze_personality_traits(transcript)
    basic_analysis["word_repetition_analysis"] = analyze_word_repetition(transcript)
    
    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ù…Ø¬
    basic_analysis["response_quality"] = evaluate_response_quality(combined_text)
    basic_analysis["hesitation_patterns"] = analyze_hesitation_patterns(combined_text)
    basic_analysis["soft_skills"] = detect_soft_skills(combined_text)
    basic_analysis["engagement_level"] = measure_engagement_level(combined_text)
    
    # Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„
    basic_analysis["comprehensive_report"] = generate_comprehensive_report(basic_analysis, transcript)
    
    return basic_analysis
